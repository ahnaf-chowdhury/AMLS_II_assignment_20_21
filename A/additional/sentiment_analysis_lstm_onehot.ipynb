{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "sentiment-analysis-lstm-onehot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "saved-contractor"
      },
      "source": [
        "# from gensim.test.utils import common_texts\n",
        "# from gensim.models import Word2Vec"
      ],
      "id": "saved-contractor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duplicate-pizza"
      },
      "source": [
        "# model = Word2Vec()"
      ],
      "id": "duplicate-pizza",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "located-cuisine"
      },
      "source": [
        "# common_texts"
      ],
      "id": "located-cuisine",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru0FNjEFYbui"
      },
      "source": [
        "# %load_ext tensorboard"
      ],
      "id": "ru0FNjEFYbui",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "intensive-portfolio"
      },
      "source": [
        "import re\n",
        "import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "intensive-portfolio",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quarterly-gasoline"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "id": "quarterly-gasoline",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5Zf93ujVDk_"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization"
      ],
      "id": "Y5Zf93ujVDk_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hearing-circumstances",
        "outputId": "47ab8364-c1e0-491b-ab77-fd431def2e60"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "id": "hearing-circumstances",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQtBCUuDrrMv",
        "outputId": "70d3e438-629b-4cb5-bdf7-c190fb9e9028"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "id": "gQtBCUuDrrMv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU0s8cpSrZuD",
        "outputId": "81a5e6fe-1b9a-4d93-ceaf-2d4f1a72019f"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "id": "SU0s8cpSrZuD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Two0zCgsT64",
        "outputId": "1e7cab6d-9d05-447e-8881-e3975ec923e6"
      },
      "source": [
        "nltk.download('tagsets')"
      ],
      "id": "0Two0zCgsT64",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meZyzdmPu94X",
        "outputId": "c5664870-b680-4141-98e7-354f67f4cdab"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "id": "meZyzdmPu94X",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ulRFaVrt0B"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "id": "s_ulRFaVrt0B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awful-halifax"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "id": "awful-halifax",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rCnKGp18E0u"
      },
      "source": [
        "from keras.preprocessing.text import one_hot"
      ],
      "id": "-rCnKGp18E0u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxNcZL4Sc7JX"
      },
      "source": [
        "# !pip install git+https://github.com/huggingface/transformers"
      ],
      "id": "LxNcZL4Sc7JX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "engaging-scanning"
      },
      "source": [
        "# from transformers import DistilBertTokenizerFast"
      ],
      "id": "engaging-scanning",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn8Yn1iPaxK8",
        "outputId": "656e99ac-caa3-4fa7-91fb-fa586d2b4a21"
      },
      "source": [
        "# make use of TPUs if available\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    \n",
        "except:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    \n",
        "print('Number of replicas in sync: ', strategy.num_replicas_in_sync)"
      ],
      "id": "mn8Yn1iPaxK8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.7.43.186:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.7.43.186:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of replicas in sync:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mny2mEans_Wa"
      },
      "source": [
        "def lemmatize_phrase(phrase):\n",
        "    # lemmatises words in a phrase, by considering their position and type\n",
        "    # words such as 'swam' and 'swimming' are changed to 'swim'\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_phrase = []\n",
        "\n",
        "    for word, tag in pos_tag(word_tokenize(phrase)):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_phrase.append(lemmatizer.lemmatize(word, pos))\n",
        "\n",
        "    return \" \".join(lemmatized_phrase)"
      ],
      "id": "mny2mEans_Wa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JByC4mM3crpH"
      },
      "source": [
        "os.mkdir('./sentiment-analysis-on-movie-reviews')"
      ],
      "id": "JByC4mM3crpH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAhbxaROa0dz"
      },
      "source": [
        "# import urllib.request\n",
        "\n",
        "# urllib.request.urlretrieve('https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data?select=train.tsv.zip', './sentiment-analysis-on-movie-reviews/train.tsv.zip')\n",
        "# urllib.request.urlretrieve('https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data?select=test.tsv.zip', './sentiment-analysis-on-movie-reviews/test.tsv.zip')"
      ],
      "id": "KAhbxaROa0dz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzLBcX53jhk_"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd"
      ],
      "id": "qzLBcX53jhk_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny_DCGEJkXNj",
        "outputId": "68ec8cc0-9842-48a5-851f-bea5462c923a"
      },
      "source": [
        "gdd.download_file_from_google_drive(file_id='1o_vzk88Fu_n5R0NRAnpvKJOIQZY-QdqB',\n",
        "                                    dest_path='./sentiment-analysis-on-movie-reviews/train.tsv',\n",
        "                                    unzip=False)\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='129VZS4q_WyJs9txnVzW85WDsCbOfbiXG',\n",
        "                                    dest_path='./sentiment-analysis-on-movie-reviews/test.tsv',\n",
        "                                    unzip=False)"
      ],
      "id": "Ny_DCGEJkXNj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1o_vzk88Fu_n5R0NRAnpvKJOIQZY-QdqB into ./sentiment-analysis-on-movie-reviews/train.tsv... Done.\n",
            "Downloading 129VZS4q_WyJs9txnVzW85WDsCbOfbiXG into ./sentiment-analysis-on-movie-reviews/test.tsv... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "assigned-lesbian"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "id": "assigned-lesbian",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "restricted-being"
      },
      "source": [
        "#this includes negative words as well, which should be removed \n",
        "\n",
        "neg = [\"aren't\", \"didn't\", \"don't\", \"doesn't\", \"hadn't\",  \"hasn't\", \"haven't\", \"isn't\", \"no\", \"not\", \"shouldn't\", \"wasn't\", \"weren't\", \"wouldn't\", \"couldn't\",\n",
        "      \"aren\", \"didn\", \"don\", \"doesn\", \"hadn\", \"hasn\", \"haven\", \"isn\", \"shouldn\", \"wasn\", \"weren\", \"wouldn\", \"couldn\",\n",
        "       \"mightn't\", \"mightn\", \"mustn't\", \"mustn\", \"needn't\", \"needn\", \"shan't\", \"shan\", \"won't\", \"won\"]\n",
        "\n",
        "# neg = ['no', 'not', \"aren\", \"didn\", \"doesn\", \"hadn\",  \"haven\", \"isn\", \"shouldn\", \"wasn\", \"weren\", \"wouldn\"]\n",
        "\n",
        "# due to the way the text is cleaned up, words like don't and haven't appear as don and haven.\n",
        "\n",
        "stop_words.difference_update(neg)\n",
        "\n",
        "# stop_words.difference_update(['no', 'not'])\n",
        "\n",
        "# stop_words"
      ],
      "id": "restricted-being",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5VJT9QnFhAG"
      },
      "source": [
        "# stop_words"
      ],
      "id": "n5VJT9QnFhAG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geographic-breeding"
      },
      "source": [
        "# returns text -> rename function\n",
        "\n",
        "def text_cleaning(text):\n",
        "    if text:\n",
        "        text = ' '.join(text.split('.'))\n",
        "        text = re.sub('\\/', ' ', text)\n",
        "        text = re.sub(r'\\\\', ' ', text)\n",
        "        text = re.sub(r'((http)\\S+)', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z]', ' ', text.strip().lower())).strip()\n",
        "        text = re.sub(r'\\W+', ' ', text.strip().lower()).strip()\n",
        "        \n",
        "        text_final = [word for word in text.split() if word not in stop_words]\n",
        "\n",
        "        if len(text_final) == 0:       # if all words are irrelevant, we may still wish to include the phrase\n",
        "            text_final = [word for word in text.split()]\n",
        "        return \" \".join(text_final)\n",
        "    return \"\""
      ],
      "id": "geographic-breeding",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deadly-cleveland",
        "outputId": "b533b428-daa2-4eed-8cef-695e000599da"
      },
      "source": [
        "import time\n",
        "\n",
        "test_input = 'my me'\n",
        "\n",
        "st = time.time()\n",
        "\n",
        "res = text_cleaning(test_input)\n",
        "\n",
        "total = time.time() - st\n",
        "\n",
        "print('{}\\n{}'.format(res, total))"
      ],
      "id": "deadly-cleveland",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my me\n",
            "0.0008487701416015625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loving-acrylic"
      },
      "source": [
        "# !jupyter nbextension enable --py --sys-prefix widgetsnbextension"
      ],
      "id": "loving-acrylic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sixth-genesis"
      },
      "source": [
        "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "id": "sixth-genesis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "minor-market"
      },
      "source": [
        "# tokenizer(['Hello i', 'hello'], truncation=True, padding=True)"
      ],
      "id": "minor-market",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "related-circulation"
      },
      "source": [
        "<h2>Data"
      ],
      "id": "related-circulation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "concrete-ghost"
      },
      "source": [
        "data_dir_train = './sentiment-analysis-on-movie-reviews/train.tsv'\n",
        "data_dir_test = './sentiment-analysis-on-movie-reviews/test.tsv'\n",
        "\n",
        "data_train = pd.read_csv(data_dir_train, sep='\\t')\n",
        "data_test = pd.read_csv(data_dir_test, sep='\\t')"
      ],
      "id": "concrete-ghost",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "architectural-tongue",
        "outputId": "0b97a0fe-d20d-44e7-cf91-3073990a3fec"
      },
      "source": [
        "data_train.head()"
      ],
      "id": "architectural-tongue",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apparent-battle"
      },
      "source": [
        "# clean dataframes:\n",
        "data_train['Phrase'] = data_train['Phrase'].apply(lambda x: text_cleaning(x))\n",
        "data_test['Phrase'] = data_test['Phrase'].apply(lambda x: text_cleaning(x))"
      ],
      "id": "apparent-battle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baI07mVKvMVf"
      },
      "source": [
        "# lemmatize data:\n",
        "# lemmatises words in a phrase, by considering their position and type\n",
        "# words such as 'swam' and 'swimming' are changed to 'swim'\n",
        "lemmatize = False\n",
        "\n",
        "if lemmatize:\n",
        "    data_train['Phrase'] = data_train['Phrase'].apply(lambda x: lemmatize_phrase(x))\n",
        "    data_test['Phrase'] = data_test['Phrase'].apply(lambda x: lemmatize_phrase(x))\n"
      ],
      "id": "baI07mVKvMVf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nearby-information",
        "outputId": "1a7021b9-e4a7-49e9-a320-c34eabccee30"
      },
      "source": [
        "data_train.head()"
      ],
      "id": "nearby-information",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapades demonstrating adage good goos...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapades demonstrating adage good goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "early-victor",
        "outputId": "fa6e77d7-6f27-4a17-bf15-a9fafdafe981"
      },
      "source": [
        "#drop duplicates from train\n",
        "\n",
        "data_train.drop_duplicates(subset = ['Phrase'], inplace = True)\n",
        "data_train.head()\n"
      ],
      "id": "early-victor",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapades demonstrating adage good goos...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapades demonstrating adage good goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades demonstrating adage good goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "5         6  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSyYfIeG5QGm",
        "outputId": "975d8d47-b26d-486e-d92c-a3f2f2f23ce2"
      },
      "source": [
        "# vocabulary size\n",
        "\n",
        "vocab = set()\n",
        "\n",
        "phrases_train = data_train['Phrase'].to_list()\n",
        "phrases_test = data_test['Phrase'].to_list()\n",
        "\n",
        "for phrase in phrases_train:\n",
        "    vocab.update(phrase.split())\n",
        "for phrase in phrases_test:\n",
        "    vocab.update(phrase.split())\n",
        "\n",
        "len(vocab)"
      ],
      "id": "eSyYfIeG5QGm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17582"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "strong-blink"
      },
      "source": [
        "# length column, explain why it's important\n",
        "\n",
        "data_train['Length'] = data_train['Phrase'].apply(lambda x: len(x.split()))\n",
        "data_test['Length'] = data_test['Phrase'].apply(lambda x: len(x.split()))"
      ],
      "id": "strong-blink",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "orange-rochester",
        "outputId": "19b187ef-a343-4c8b-f50b-5c53b2b2f119"
      },
      "source": [
        "data_train.head()"
      ],
      "id": "orange-rochester",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapades demonstrating adage good goos...</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>series escapades demonstrating adage good goose</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades demonstrating adage good goose</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId  ... Sentiment  Length\n",
              "0         1           1  ...         1      15\n",
              "1         2           1  ...         2       6\n",
              "2         3           1  ...         2       1\n",
              "3         4           1  ...         2       1\n",
              "5         6           1  ...         2       5\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z5ID7GYNtuR"
      },
      "source": [
        "maxlen = 200"
      ],
      "id": "_Z5ID7GYNtuR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "burning-sociology"
      },
      "source": [
        "Train-Test-Validation Split"
      ],
      "id": "burning-sociology"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affected-vatican"
      },
      "source": [
        "validation_split = 0.15\n",
        "test_split = 0.15\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    data_train[data_train['Length']>0]['Phrase'],\n",
        "    data_train[data_train['Length']>0]['Sentiment'],\n",
        "    test_size = test_split,\n",
        "    stratify = data_train[data_train['Length'] > 0]['Sentiment'],\n",
        "    random_state = 40)\n",
        "\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    test_size = validation_split,\n",
        "    stratify = y_train,\n",
        "    random_state = 40)\n",
        "\n",
        "# ('Length']>0 filters out 'empty' phrases, which were previously punctuation marks)"
      ],
      "id": "affected-vatican",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "funded-yesterday",
        "outputId": "7943868d-d07d-4b19-e7df-d29523e3ee36"
      },
      "source": [
        "x_train.shape"
      ],
      "id": "funded-yesterday",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61816,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accepting-stamp"
      },
      "source": [
        "# encodings_train = tokenizer(x_train.tolist(), truncation = True, padding = True)\n",
        "# encodings_validation = tokenizer(x_validation.tolist(), truncation = True, padding = True)"
      ],
      "id": "accepting-stamp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8OaZQApPTzu"
      },
      "source": [
        "vocab_size = 20000\n",
        "\n",
        "encodings_train = x_train.apply(lambda z: one_hot(z, vocab_size))\n",
        "encodings_validation = x_validation.apply(lambda z: one_hot(z, vocab_size))\n",
        "encodings_test = x_test.apply(lambda z: one_hot(z, vocab_size))"
      ],
      "id": "z8OaZQApPTzu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTkca570-pZT"
      },
      "source": [
        "max_len_in_train = len(max(encodings_train, key = lambda i: len(i)))\n",
        "max_len_in_validation = len(max(encodings_validation, key = lambda i: len(i)))\n",
        "max_len_in_test = len(max(encodings_test, key = lambda i: len(i)))\n",
        "max_len_in_dataset = max(max_len_in_train, max_len_in_validation, max_len_in_test)\n",
        "\n",
        "if max_len_in_dataset < maxlen:\n",
        "    maxlen = max_len_in_dataset"
      ],
      "id": "cTkca570-pZT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGl5J5hEAZBd"
      },
      "source": [
        "encodings_train = keras.preprocessing.sequence.pad_sequences(encodings_train, maxlen=maxlen)\n",
        "encodings_validation = keras.preprocessing.sequence.pad_sequences(encodings_validation, maxlen=maxlen)\n",
        "encodings_test = keras.preprocessing.sequence.pad_sequences(encodings_test, maxlen=maxlen)"
      ],
      "id": "KGl5J5hEAZBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "divided-texas"
      },
      "source": [
        "Train and Validation Datasets"
      ],
      "id": "divided-texas"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chubby-folks"
      },
      "source": [
        "batch_size = 512\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    encodings_train, y_train.values)).shuffle(10000).batch(batch_size).repeat()\n",
        "\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "#     encodings_train, y_train.values)).shuffle(10000).batch(32)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    encodings_validation, y_validation.values)).shuffle(10000).batch(batch_size)"
      ],
      "id": "chubby-folks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcJa7EkjaMBk"
      },
      "source": [
        "# logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ],
      "id": "GcJa7EkjaMBk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aToGdqzAuMSv"
      },
      "source": [
        "# %tensorboard --logdir logs"
      ],
      "id": "aToGdqzAuMSv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cardiac-rotation"
      },
      "source": [
        "Model"
      ],
      "id": "cardiac-rotation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8xBGEENONCa",
        "outputId": "9c15fdc9-f943-4753-b9c4-f6f5f98400b0"
      },
      "source": [
        "embedding_dims = 128\n",
        "\n",
        "with strategy.scope():\n",
        "    model = Sequential()\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "    model.add(inputs)\n",
        "    model.add(Embedding(vocab_size, embedding_dims, trainable=False))\n",
        "\n",
        "    model.add(Bidirectional(LSTM(embedding_dims//2, return_sequences=True, activation='softmax')))\n",
        "    # model.add(Bidirectional(LSTM(embedding_dims//2, return_sequences=True)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Bidirectional(LSTM(embedding_dims//2)))\n",
        "    # model.add(Bidirectional(LSTM(embedding_dims//2, activation='softmax')))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dense(5, activation=\"sigmoid\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "    training_st = time.time()\n",
        "\n",
        "    history = model.fit(train_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=335,\n",
        "                        validation_steps = len(x_validation) // batch_size,\n",
        "                        steps_per_epoch = len(x_train) // batch_size,\n",
        "                        validation_data = val_dataset)\n",
        "    \n",
        "    training_tot = time.time() - training_st"
      ],
      "id": "u8xBGEENONCa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 128)         2560000   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, None, 128)         98816     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, None, 128)         512       \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 2,759,301\n",
            "Trainable params: 198,789\n",
            "Non-trainable params: 2,560,512\n",
            "_________________________________________________________________\n",
            "Epoch 1/335\n",
            "120/120 [==============================] - 15s 49ms/step - loss: 1.6815 - accuracy: 0.3110 - val_loss: 1.2876 - val_accuracy: 0.5113\n",
            "Epoch 2/335\n",
            "120/120 [==============================] - 5s 26ms/step - loss: 1.2918 - accuracy: 0.4982 - val_loss: 1.2815 - val_accuracy: 0.5105\n",
            "Epoch 3/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2603 - accuracy: 0.5041 - val_loss: 1.2840 - val_accuracy: 0.5090\n",
            "Epoch 4/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2483 - accuracy: 0.5098 - val_loss: 1.2753 - val_accuracy: 0.5099\n",
            "Epoch 5/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2472 - accuracy: 0.5047 - val_loss: 1.2581 - val_accuracy: 0.5107\n",
            "Epoch 6/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2330 - accuracy: 0.5112 - val_loss: 1.2439 - val_accuracy: 0.5109\n",
            "Epoch 7/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2349 - accuracy: 0.5097 - val_loss: 1.2264 - val_accuracy: 0.5105\n",
            "Epoch 8/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2335 - accuracy: 0.5119 - val_loss: 1.2239 - val_accuracy: 0.5113\n",
            "Epoch 9/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2323 - accuracy: 0.5111 - val_loss: 1.2241 - val_accuracy: 0.5130\n",
            "Epoch 10/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2287 - accuracy: 0.5096 - val_loss: 1.2230 - val_accuracy: 0.5126\n",
            "Epoch 11/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2280 - accuracy: 0.5096 - val_loss: 1.2222 - val_accuracy: 0.5108\n",
            "Epoch 12/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2292 - accuracy: 0.5099 - val_loss: 1.2236 - val_accuracy: 0.5120\n",
            "Epoch 13/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2252 - accuracy: 0.5128 - val_loss: 1.2243 - val_accuracy: 0.5121\n",
            "Epoch 14/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2288 - accuracy: 0.5112 - val_loss: 1.2246 - val_accuracy: 0.5106\n",
            "Epoch 15/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2229 - accuracy: 0.5134 - val_loss: 1.2207 - val_accuracy: 0.5127\n",
            "Epoch 16/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2218 - accuracy: 0.5131 - val_loss: 1.2208 - val_accuracy: 0.5126\n",
            "Epoch 17/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2310 - accuracy: 0.5092 - val_loss: 1.2217 - val_accuracy: 0.5113\n",
            "Epoch 18/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2244 - accuracy: 0.5113 - val_loss: 1.2206 - val_accuracy: 0.5132\n",
            "Epoch 19/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2281 - accuracy: 0.5116 - val_loss: 1.2204 - val_accuracy: 0.5117\n",
            "Epoch 20/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2224 - accuracy: 0.5114 - val_loss: 1.2192 - val_accuracy: 0.5119\n",
            "Epoch 21/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2210 - accuracy: 0.5135 - val_loss: 1.2189 - val_accuracy: 0.5107\n",
            "Epoch 22/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2269 - accuracy: 0.5109 - val_loss: 1.2199 - val_accuracy: 0.5126\n",
            "Epoch 23/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2208 - accuracy: 0.5140 - val_loss: 1.2190 - val_accuracy: 0.5118\n",
            "Epoch 24/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2200 - accuracy: 0.5141 - val_loss: 1.2199 - val_accuracy: 0.5124\n",
            "Epoch 25/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2231 - accuracy: 0.5108 - val_loss: 1.2180 - val_accuracy: 0.5133\n",
            "Epoch 26/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2246 - accuracy: 0.5108 - val_loss: 1.2158 - val_accuracy: 0.5132\n",
            "Epoch 27/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2185 - accuracy: 0.5152 - val_loss: 1.2153 - val_accuracy: 0.5126\n",
            "Epoch 28/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2184 - accuracy: 0.5133 - val_loss: 1.2117 - val_accuracy: 0.5167\n",
            "Epoch 29/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2171 - accuracy: 0.5133 - val_loss: 1.2149 - val_accuracy: 0.5130\n",
            "Epoch 30/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2223 - accuracy: 0.5110 - val_loss: 1.2104 - val_accuracy: 0.5126\n",
            "Epoch 31/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2090 - accuracy: 0.5179 - val_loss: 1.2091 - val_accuracy: 0.5151\n",
            "Epoch 32/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2189 - accuracy: 0.5132 - val_loss: 1.2065 - val_accuracy: 0.5169\n",
            "Epoch 33/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2203 - accuracy: 0.5111 - val_loss: 1.2098 - val_accuracy: 0.5148\n",
            "Epoch 34/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2113 - accuracy: 0.5182 - val_loss: 1.2055 - val_accuracy: 0.5163\n",
            "Epoch 35/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2104 - accuracy: 0.5193 - val_loss: 1.2018 - val_accuracy: 0.5185\n",
            "Epoch 36/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2094 - accuracy: 0.5158 - val_loss: 1.2037 - val_accuracy: 0.5159\n",
            "Epoch 37/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2094 - accuracy: 0.5164 - val_loss: 1.2028 - val_accuracy: 0.5179\n",
            "Epoch 38/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2084 - accuracy: 0.5186 - val_loss: 1.2013 - val_accuracy: 0.5192\n",
            "Epoch 39/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2096 - accuracy: 0.5202 - val_loss: 1.1999 - val_accuracy: 0.5174\n",
            "Epoch 40/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2053 - accuracy: 0.5175 - val_loss: 1.2001 - val_accuracy: 0.5176\n",
            "Epoch 41/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2075 - accuracy: 0.5170 - val_loss: 1.2018 - val_accuracy: 0.5166\n",
            "Epoch 42/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2075 - accuracy: 0.5186 - val_loss: 1.1981 - val_accuracy: 0.5186\n",
            "Epoch 43/335\n",
            "120/120 [==============================] - 4s 30ms/step - loss: 1.2035 - accuracy: 0.5211 - val_loss: 1.2030 - val_accuracy: 0.5192\n",
            "Epoch 44/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2064 - accuracy: 0.5190 - val_loss: 1.1962 - val_accuracy: 0.5187\n",
            "Epoch 45/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1987 - accuracy: 0.5221 - val_loss: 1.1970 - val_accuracy: 0.5181\n",
            "Epoch 46/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.2042 - accuracy: 0.5192 - val_loss: 1.1991 - val_accuracy: 0.5174\n",
            "Epoch 47/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2012 - accuracy: 0.5212 - val_loss: 1.2002 - val_accuracy: 0.5194\n",
            "Epoch 48/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.2011 - accuracy: 0.5209 - val_loss: 1.1978 - val_accuracy: 0.5214\n",
            "Epoch 49/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1962 - accuracy: 0.5208 - val_loss: 1.1971 - val_accuracy: 0.5199\n",
            "Epoch 50/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1970 - accuracy: 0.5267 - val_loss: 1.1979 - val_accuracy: 0.5215\n",
            "Epoch 51/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1929 - accuracy: 0.5260 - val_loss: 1.1965 - val_accuracy: 0.5220\n",
            "Epoch 52/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1956 - accuracy: 0.5240 - val_loss: 1.1963 - val_accuracy: 0.5218\n",
            "Epoch 53/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1914 - accuracy: 0.5253 - val_loss: 1.1929 - val_accuracy: 0.5230\n",
            "Epoch 54/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1899 - accuracy: 0.5279 - val_loss: 1.1985 - val_accuracy: 0.5181\n",
            "Epoch 55/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1922 - accuracy: 0.5254 - val_loss: 1.2020 - val_accuracy: 0.5223\n",
            "Epoch 56/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1954 - accuracy: 0.5239 - val_loss: 1.1905 - val_accuracy: 0.5245\n",
            "Epoch 57/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1839 - accuracy: 0.5299 - val_loss: 1.1933 - val_accuracy: 0.5232\n",
            "Epoch 58/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1948 - accuracy: 0.5255 - val_loss: 1.1924 - val_accuracy: 0.5235\n",
            "Epoch 59/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1891 - accuracy: 0.5288 - val_loss: 1.2016 - val_accuracy: 0.5190\n",
            "Epoch 60/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1877 - accuracy: 0.5275 - val_loss: 1.1937 - val_accuracy: 0.5226\n",
            "Epoch 61/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1852 - accuracy: 0.5289 - val_loss: 1.1934 - val_accuracy: 0.5220\n",
            "Epoch 62/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1829 - accuracy: 0.5312 - val_loss: 1.1917 - val_accuracy: 0.5236\n",
            "Epoch 63/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1864 - accuracy: 0.5269 - val_loss: 1.1891 - val_accuracy: 0.5261\n",
            "Epoch 64/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1812 - accuracy: 0.5275 - val_loss: 1.1916 - val_accuracy: 0.5263\n",
            "Epoch 65/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1815 - accuracy: 0.5301 - val_loss: 1.1900 - val_accuracy: 0.5233\n",
            "Epoch 66/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1787 - accuracy: 0.5320 - val_loss: 1.1841 - val_accuracy: 0.5259\n",
            "Epoch 67/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1784 - accuracy: 0.5314 - val_loss: 1.1906 - val_accuracy: 0.5233\n",
            "Epoch 68/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1835 - accuracy: 0.5306 - val_loss: 1.1901 - val_accuracy: 0.5229\n",
            "Epoch 69/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1803 - accuracy: 0.5290 - val_loss: 1.1900 - val_accuracy: 0.5246\n",
            "Epoch 70/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1778 - accuracy: 0.5312 - val_loss: 1.1893 - val_accuracy: 0.5269\n",
            "Epoch 71/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1760 - accuracy: 0.5320 - val_loss: 1.1858 - val_accuracy: 0.5266\n",
            "Epoch 72/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1813 - accuracy: 0.5302 - val_loss: 1.1840 - val_accuracy: 0.5273\n",
            "Epoch 73/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1793 - accuracy: 0.5304 - val_loss: 1.1826 - val_accuracy: 0.5282\n",
            "Epoch 74/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1784 - accuracy: 0.5309 - val_loss: 1.1858 - val_accuracy: 0.5273\n",
            "Epoch 75/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1755 - accuracy: 0.5319 - val_loss: 1.1880 - val_accuracy: 0.5260\n",
            "Epoch 76/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1760 - accuracy: 0.5301 - val_loss: 1.1873 - val_accuracy: 0.5257\n",
            "Epoch 77/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1709 - accuracy: 0.5332 - val_loss: 1.1883 - val_accuracy: 0.5250\n",
            "Epoch 78/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1749 - accuracy: 0.5302 - val_loss: 1.1823 - val_accuracy: 0.5267\n",
            "Epoch 79/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1808 - accuracy: 0.5305 - val_loss: 1.1822 - val_accuracy: 0.5284\n",
            "Epoch 80/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1756 - accuracy: 0.5313 - val_loss: 1.1805 - val_accuracy: 0.5274\n",
            "Epoch 81/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1666 - accuracy: 0.5333 - val_loss: 1.1844 - val_accuracy: 0.5288\n",
            "Epoch 82/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1654 - accuracy: 0.5346 - val_loss: 1.1821 - val_accuracy: 0.5270\n",
            "Epoch 83/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1665 - accuracy: 0.5364 - val_loss: 1.1823 - val_accuracy: 0.5281\n",
            "Epoch 84/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1688 - accuracy: 0.5361 - val_loss: 1.1775 - val_accuracy: 0.5288\n",
            "Epoch 85/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1699 - accuracy: 0.5337 - val_loss: 1.1853 - val_accuracy: 0.5267\n",
            "Epoch 86/335\n",
            "120/120 [==============================] - 4s 32ms/step - loss: 1.1693 - accuracy: 0.5315 - val_loss: 1.1789 - val_accuracy: 0.5290\n",
            "Epoch 87/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1665 - accuracy: 0.5350 - val_loss: 1.1832 - val_accuracy: 0.5272\n",
            "Epoch 88/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1653 - accuracy: 0.5366 - val_loss: 1.1781 - val_accuracy: 0.5292\n",
            "Epoch 89/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1677 - accuracy: 0.5322 - val_loss: 1.1810 - val_accuracy: 0.5289\n",
            "Epoch 90/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1607 - accuracy: 0.5387 - val_loss: 1.1760 - val_accuracy: 0.5287\n",
            "Epoch 91/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1596 - accuracy: 0.5394 - val_loss: 1.1754 - val_accuracy: 0.5282\n",
            "Epoch 92/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1687 - accuracy: 0.5331 - val_loss: 1.1814 - val_accuracy: 0.5259\n",
            "Epoch 93/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1622 - accuracy: 0.5398 - val_loss: 1.1789 - val_accuracy: 0.5272\n",
            "Epoch 94/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1599 - accuracy: 0.5383 - val_loss: 1.1733 - val_accuracy: 0.5310\n",
            "Epoch 95/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1644 - accuracy: 0.5333 - val_loss: 1.1760 - val_accuracy: 0.5292\n",
            "Epoch 96/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1594 - accuracy: 0.5405 - val_loss: 1.1796 - val_accuracy: 0.5279\n",
            "Epoch 97/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1643 - accuracy: 0.5354 - val_loss: 1.1853 - val_accuracy: 0.5271\n",
            "Epoch 98/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1578 - accuracy: 0.5390 - val_loss: 1.1787 - val_accuracy: 0.5292\n",
            "Epoch 99/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1570 - accuracy: 0.5401 - val_loss: 1.1753 - val_accuracy: 0.5292\n",
            "Epoch 100/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1554 - accuracy: 0.5389 - val_loss: 1.1718 - val_accuracy: 0.5285\n",
            "Epoch 101/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1574 - accuracy: 0.5394 - val_loss: 1.1704 - val_accuracy: 0.5293\n",
            "Epoch 102/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1523 - accuracy: 0.5407 - val_loss: 1.1765 - val_accuracy: 0.5295\n",
            "Epoch 103/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1528 - accuracy: 0.5404 - val_loss: 1.1766 - val_accuracy: 0.5277\n",
            "Epoch 104/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1518 - accuracy: 0.5421 - val_loss: 1.1738 - val_accuracy: 0.5291\n",
            "Epoch 105/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1554 - accuracy: 0.5406 - val_loss: 1.1756 - val_accuracy: 0.5271\n",
            "Epoch 106/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1495 - accuracy: 0.5427 - val_loss: 1.1746 - val_accuracy: 0.5285\n",
            "Epoch 107/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1502 - accuracy: 0.5404 - val_loss: 1.1686 - val_accuracy: 0.5308\n",
            "Epoch 108/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1480 - accuracy: 0.5417 - val_loss: 1.1699 - val_accuracy: 0.5308\n",
            "Epoch 109/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1554 - accuracy: 0.5382 - val_loss: 1.1698 - val_accuracy: 0.5309\n",
            "Epoch 110/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1483 - accuracy: 0.5393 - val_loss: 1.1704 - val_accuracy: 0.5299\n",
            "Epoch 111/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1510 - accuracy: 0.5403 - val_loss: 1.1772 - val_accuracy: 0.5272\n",
            "Epoch 112/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1487 - accuracy: 0.5443 - val_loss: 1.1726 - val_accuracy: 0.5298\n",
            "Epoch 113/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1505 - accuracy: 0.5396 - val_loss: 1.1611 - val_accuracy: 0.5334\n",
            "Epoch 114/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1488 - accuracy: 0.5439 - val_loss: 1.1679 - val_accuracy: 0.5324\n",
            "Epoch 115/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1462 - accuracy: 0.5435 - val_loss: 1.1650 - val_accuracy: 0.5314\n",
            "Epoch 116/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1419 - accuracy: 0.5445 - val_loss: 1.1670 - val_accuracy: 0.5345\n",
            "Epoch 117/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1453 - accuracy: 0.5455 - val_loss: 1.1667 - val_accuracy: 0.5329\n",
            "Epoch 118/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1474 - accuracy: 0.5440 - val_loss: 1.1649 - val_accuracy: 0.5328\n",
            "Epoch 119/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1421 - accuracy: 0.5446 - val_loss: 1.1640 - val_accuracy: 0.5366\n",
            "Epoch 120/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1455 - accuracy: 0.5458 - val_loss: 1.1619 - val_accuracy: 0.5343\n",
            "Epoch 121/335\n",
            "120/120 [==============================] - 4s 31ms/step - loss: 1.1441 - accuracy: 0.5439 - val_loss: 1.1655 - val_accuracy: 0.5317\n",
            "Epoch 122/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1443 - accuracy: 0.5436 - val_loss: 1.1666 - val_accuracy: 0.5320\n",
            "Epoch 123/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1381 - accuracy: 0.5469 - val_loss: 1.1714 - val_accuracy: 0.5316\n",
            "Epoch 124/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1395 - accuracy: 0.5456 - val_loss: 1.1644 - val_accuracy: 0.5328\n",
            "Epoch 125/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1359 - accuracy: 0.5492 - val_loss: 1.1585 - val_accuracy: 0.5366\n",
            "Epoch 126/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1349 - accuracy: 0.5516 - val_loss: 1.1575 - val_accuracy: 0.5371\n",
            "Epoch 127/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1360 - accuracy: 0.5492 - val_loss: 1.1615 - val_accuracy: 0.5326\n",
            "Epoch 128/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1329 - accuracy: 0.5464 - val_loss: 1.1543 - val_accuracy: 0.5398\n",
            "Epoch 129/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1379 - accuracy: 0.5486 - val_loss: 1.1573 - val_accuracy: 0.5341\n",
            "Epoch 130/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1316 - accuracy: 0.5505 - val_loss: 1.1618 - val_accuracy: 0.5329\n",
            "Epoch 131/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1330 - accuracy: 0.5478 - val_loss: 1.1576 - val_accuracy: 0.5360\n",
            "Epoch 132/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1362 - accuracy: 0.5487 - val_loss: 1.1591 - val_accuracy: 0.5372\n",
            "Epoch 133/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1308 - accuracy: 0.5516 - val_loss: 1.1537 - val_accuracy: 0.5348\n",
            "Epoch 134/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1368 - accuracy: 0.5458 - val_loss: 1.1533 - val_accuracy: 0.5371\n",
            "Epoch 135/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1300 - accuracy: 0.5517 - val_loss: 1.1551 - val_accuracy: 0.5334\n",
            "Epoch 136/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1384 - accuracy: 0.5500 - val_loss: 1.1567 - val_accuracy: 0.5354\n",
            "Epoch 137/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1302 - accuracy: 0.5493 - val_loss: 1.1528 - val_accuracy: 0.5382\n",
            "Epoch 138/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1288 - accuracy: 0.5518 - val_loss: 1.1600 - val_accuracy: 0.5316\n",
            "Epoch 139/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1295 - accuracy: 0.5501 - val_loss: 1.1564 - val_accuracy: 0.5357\n",
            "Epoch 140/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1265 - accuracy: 0.5541 - val_loss: 1.1555 - val_accuracy: 0.5372\n",
            "Epoch 141/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1229 - accuracy: 0.5536 - val_loss: 1.1508 - val_accuracy: 0.5386\n",
            "Epoch 142/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1253 - accuracy: 0.5527 - val_loss: 1.1556 - val_accuracy: 0.5366\n",
            "Epoch 143/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1269 - accuracy: 0.5522 - val_loss: 1.1505 - val_accuracy: 0.5421\n",
            "Epoch 144/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1265 - accuracy: 0.5520 - val_loss: 1.1479 - val_accuracy: 0.5406\n",
            "Epoch 145/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1242 - accuracy: 0.5538 - val_loss: 1.1526 - val_accuracy: 0.5398\n",
            "Epoch 146/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1263 - accuracy: 0.5534 - val_loss: 1.1564 - val_accuracy: 0.5366\n",
            "Epoch 147/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1240 - accuracy: 0.5552 - val_loss: 1.1580 - val_accuracy: 0.5363\n",
            "Epoch 148/335\n",
            "120/120 [==============================] - 3s 25ms/step - loss: 1.1266 - accuracy: 0.5528 - val_loss: 1.1488 - val_accuracy: 0.5359\n",
            "Epoch 149/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1276 - accuracy: 0.5530 - val_loss: 1.1503 - val_accuracy: 0.5396\n",
            "Epoch 150/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1258 - accuracy: 0.5516 - val_loss: 1.1585 - val_accuracy: 0.5335\n",
            "Epoch 151/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1162 - accuracy: 0.5571 - val_loss: 1.1486 - val_accuracy: 0.5420\n",
            "Epoch 152/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1242 - accuracy: 0.5539 - val_loss: 1.1515 - val_accuracy: 0.5380\n",
            "Epoch 153/335\n",
            "120/120 [==============================] - 4s 31ms/step - loss: 1.1187 - accuracy: 0.5561 - val_loss: 1.1535 - val_accuracy: 0.5389\n",
            "Epoch 154/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1175 - accuracy: 0.5549 - val_loss: 1.1532 - val_accuracy: 0.5408\n",
            "Epoch 155/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1176 - accuracy: 0.5580 - val_loss: 1.1613 - val_accuracy: 0.5390\n",
            "Epoch 156/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1165 - accuracy: 0.5587 - val_loss: 1.1512 - val_accuracy: 0.5405\n",
            "Epoch 157/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1156 - accuracy: 0.5570 - val_loss: 1.1517 - val_accuracy: 0.5410\n",
            "Epoch 158/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1153 - accuracy: 0.5572 - val_loss: 1.1548 - val_accuracy: 0.5413\n",
            "Epoch 159/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1188 - accuracy: 0.5558 - val_loss: 1.1585 - val_accuracy: 0.5379\n",
            "Epoch 160/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1174 - accuracy: 0.5567 - val_loss: 1.1533 - val_accuracy: 0.5407\n",
            "Epoch 161/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1193 - accuracy: 0.5572 - val_loss: 1.1515 - val_accuracy: 0.5411\n",
            "Epoch 162/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1082 - accuracy: 0.5613 - val_loss: 1.1591 - val_accuracy: 0.5416\n",
            "Epoch 163/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1138 - accuracy: 0.5593 - val_loss: 1.1596 - val_accuracy: 0.5401\n",
            "Epoch 164/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1104 - accuracy: 0.5629 - val_loss: 1.1538 - val_accuracy: 0.5409\n",
            "Epoch 165/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1125 - accuracy: 0.5590 - val_loss: 1.1533 - val_accuracy: 0.5411\n",
            "Epoch 166/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1116 - accuracy: 0.5604 - val_loss: 1.1462 - val_accuracy: 0.5445\n",
            "Epoch 167/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1030 - accuracy: 0.5612 - val_loss: 1.1564 - val_accuracy: 0.5423\n",
            "Epoch 168/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1156 - accuracy: 0.5578 - val_loss: 1.1545 - val_accuracy: 0.5422\n",
            "Epoch 169/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1073 - accuracy: 0.5624 - val_loss: 1.1447 - val_accuracy: 0.5454\n",
            "Epoch 170/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.1065 - accuracy: 0.5646 - val_loss: 1.1425 - val_accuracy: 0.5470\n",
            "Epoch 171/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1085 - accuracy: 0.5618 - val_loss: 1.1580 - val_accuracy: 0.5430\n",
            "Epoch 172/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1053 - accuracy: 0.5617 - val_loss: 1.1463 - val_accuracy: 0.5445\n",
            "Epoch 173/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1069 - accuracy: 0.5632 - val_loss: 1.1424 - val_accuracy: 0.5425\n",
            "Epoch 174/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1003 - accuracy: 0.5637 - val_loss: 1.1517 - val_accuracy: 0.5429\n",
            "Epoch 175/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1030 - accuracy: 0.5648 - val_loss: 1.1452 - val_accuracy: 0.5449\n",
            "Epoch 176/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1053 - accuracy: 0.5611 - val_loss: 1.1435 - val_accuracy: 0.5470\n",
            "Epoch 177/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1013 - accuracy: 0.5656 - val_loss: 1.1477 - val_accuracy: 0.5457\n",
            "Epoch 178/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0994 - accuracy: 0.5638 - val_loss: 1.1530 - val_accuracy: 0.5433\n",
            "Epoch 179/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.1015 - accuracy: 0.5644 - val_loss: 1.1470 - val_accuracy: 0.5447\n",
            "Epoch 180/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0978 - accuracy: 0.5640 - val_loss: 1.1401 - val_accuracy: 0.5450\n",
            "Epoch 181/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0957 - accuracy: 0.5681 - val_loss: 1.1411 - val_accuracy: 0.5467\n",
            "Epoch 182/335\n",
            "120/120 [==============================] - 4s 31ms/step - loss: 1.0947 - accuracy: 0.5679 - val_loss: 1.1419 - val_accuracy: 0.5477\n",
            "Epoch 183/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0911 - accuracy: 0.5695 - val_loss: 1.1552 - val_accuracy: 0.5453\n",
            "Epoch 184/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0954 - accuracy: 0.5659 - val_loss: 1.1437 - val_accuracy: 0.5475\n",
            "Epoch 185/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0907 - accuracy: 0.5675 - val_loss: 1.1496 - val_accuracy: 0.5473\n",
            "Epoch 186/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0938 - accuracy: 0.5657 - val_loss: 1.1480 - val_accuracy: 0.5467\n",
            "Epoch 187/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0948 - accuracy: 0.5636 - val_loss: 1.1485 - val_accuracy: 0.5458\n",
            "Epoch 188/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0897 - accuracy: 0.5692 - val_loss: 1.1420 - val_accuracy: 0.5472\n",
            "Epoch 189/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0900 - accuracy: 0.5689 - val_loss: 1.1409 - val_accuracy: 0.5470\n",
            "Epoch 190/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0890 - accuracy: 0.5688 - val_loss: 1.1424 - val_accuracy: 0.5499\n",
            "Epoch 191/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0860 - accuracy: 0.5699 - val_loss: 1.1398 - val_accuracy: 0.5496\n",
            "Epoch 192/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0803 - accuracy: 0.5742 - val_loss: 1.1401 - val_accuracy: 0.5475\n",
            "Epoch 193/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0813 - accuracy: 0.5723 - val_loss: 1.1423 - val_accuracy: 0.5500\n",
            "Epoch 194/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0821 - accuracy: 0.5690 - val_loss: 1.1471 - val_accuracy: 0.5472\n",
            "Epoch 195/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0813 - accuracy: 0.5695 - val_loss: 1.1434 - val_accuracy: 0.5499\n",
            "Epoch 196/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0817 - accuracy: 0.5694 - val_loss: 1.1443 - val_accuracy: 0.5495\n",
            "Epoch 197/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0836 - accuracy: 0.5716 - val_loss: 1.1464 - val_accuracy: 0.5483\n",
            "Epoch 198/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0785 - accuracy: 0.5740 - val_loss: 1.1431 - val_accuracy: 0.5500\n",
            "Epoch 199/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0818 - accuracy: 0.5721 - val_loss: 1.1379 - val_accuracy: 0.5514\n",
            "Epoch 200/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0805 - accuracy: 0.5715 - val_loss: 1.1453 - val_accuracy: 0.5500\n",
            "Epoch 201/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0770 - accuracy: 0.5755 - val_loss: 1.1478 - val_accuracy: 0.5472\n",
            "Epoch 202/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0806 - accuracy: 0.5724 - val_loss: 1.1385 - val_accuracy: 0.5508\n",
            "Epoch 203/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0789 - accuracy: 0.5722 - val_loss: 1.1359 - val_accuracy: 0.5518\n",
            "Epoch 204/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0718 - accuracy: 0.5777 - val_loss: 1.1401 - val_accuracy: 0.5492\n",
            "Epoch 205/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0723 - accuracy: 0.5769 - val_loss: 1.1388 - val_accuracy: 0.5544\n",
            "Epoch 206/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0702 - accuracy: 0.5765 - val_loss: 1.1443 - val_accuracy: 0.5503\n",
            "Epoch 207/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0725 - accuracy: 0.5743 - val_loss: 1.1392 - val_accuracy: 0.5516\n",
            "Epoch 208/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0627 - accuracy: 0.5781 - val_loss: 1.1412 - val_accuracy: 0.5490\n",
            "Epoch 209/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0699 - accuracy: 0.5753 - val_loss: 1.1433 - val_accuracy: 0.5509\n",
            "Epoch 210/335\n",
            "120/120 [==============================] - 4s 32ms/step - loss: 1.0766 - accuracy: 0.5710 - val_loss: 1.1317 - val_accuracy: 0.5527\n",
            "Epoch 211/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0651 - accuracy: 0.5789 - val_loss: 1.1436 - val_accuracy: 0.5494\n",
            "Epoch 212/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0718 - accuracy: 0.5747 - val_loss: 1.1361 - val_accuracy: 0.5510\n",
            "Epoch 213/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0618 - accuracy: 0.5780 - val_loss: 1.1357 - val_accuracy: 0.5544\n",
            "Epoch 214/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0630 - accuracy: 0.5789 - val_loss: 1.1419 - val_accuracy: 0.5523\n",
            "Epoch 215/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0677 - accuracy: 0.5773 - val_loss: 1.1454 - val_accuracy: 0.5496\n",
            "Epoch 216/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0630 - accuracy: 0.5795 - val_loss: 1.1374 - val_accuracy: 0.5522\n",
            "Epoch 217/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0638 - accuracy: 0.5799 - val_loss: 1.1341 - val_accuracy: 0.5513\n",
            "Epoch 218/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0647 - accuracy: 0.5780 - val_loss: 1.1389 - val_accuracy: 0.5524\n",
            "Epoch 219/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0580 - accuracy: 0.5807 - val_loss: 1.1465 - val_accuracy: 0.5458\n",
            "Epoch 220/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0563 - accuracy: 0.5795 - val_loss: 1.1394 - val_accuracy: 0.5515\n",
            "Epoch 221/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0575 - accuracy: 0.5799 - val_loss: 1.1433 - val_accuracy: 0.5525\n",
            "Epoch 222/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0592 - accuracy: 0.5798 - val_loss: 1.1351 - val_accuracy: 0.5531\n",
            "Epoch 223/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0557 - accuracy: 0.5830 - val_loss: 1.1443 - val_accuracy: 0.5508\n",
            "Epoch 224/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0582 - accuracy: 0.5825 - val_loss: 1.1344 - val_accuracy: 0.5540\n",
            "Epoch 225/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0517 - accuracy: 0.5818 - val_loss: 1.1457 - val_accuracy: 0.5518\n",
            "Epoch 226/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0505 - accuracy: 0.5806 - val_loss: 1.1313 - val_accuracy: 0.5552\n",
            "Epoch 227/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0482 - accuracy: 0.5828 - val_loss: 1.1394 - val_accuracy: 0.5524\n",
            "Epoch 228/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0496 - accuracy: 0.5822 - val_loss: 1.1346 - val_accuracy: 0.5557\n",
            "Epoch 229/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0509 - accuracy: 0.5849 - val_loss: 1.1486 - val_accuracy: 0.5512\n",
            "Epoch 230/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0539 - accuracy: 0.5794 - val_loss: 1.1324 - val_accuracy: 0.5508\n",
            "Epoch 231/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0519 - accuracy: 0.5806 - val_loss: 1.1388 - val_accuracy: 0.5535\n",
            "Epoch 232/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0518 - accuracy: 0.5805 - val_loss: 1.1319 - val_accuracy: 0.5567\n",
            "Epoch 233/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0460 - accuracy: 0.5875 - val_loss: 1.1347 - val_accuracy: 0.5532\n",
            "Epoch 234/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0468 - accuracy: 0.5828 - val_loss: 1.1295 - val_accuracy: 0.5539\n",
            "Epoch 235/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0484 - accuracy: 0.5833 - val_loss: 1.1369 - val_accuracy: 0.5552\n",
            "Epoch 236/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0446 - accuracy: 0.5861 - val_loss: 1.1327 - val_accuracy: 0.5545\n",
            "Epoch 237/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0461 - accuracy: 0.5877 - val_loss: 1.1255 - val_accuracy: 0.5578\n",
            "Epoch 238/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0465 - accuracy: 0.5853 - val_loss: 1.1348 - val_accuracy: 0.5551\n",
            "Epoch 239/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0424 - accuracy: 0.5877 - val_loss: 1.1368 - val_accuracy: 0.5546\n",
            "Epoch 240/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0444 - accuracy: 0.5825 - val_loss: 1.1242 - val_accuracy: 0.5603\n",
            "Epoch 241/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0392 - accuracy: 0.5891 - val_loss: 1.1230 - val_accuracy: 0.5572\n",
            "Epoch 242/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0400 - accuracy: 0.5871 - val_loss: 1.1415 - val_accuracy: 0.5572\n",
            "Epoch 243/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0427 - accuracy: 0.5884 - val_loss: 1.1349 - val_accuracy: 0.5558\n",
            "Epoch 244/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0363 - accuracy: 0.5861 - val_loss: 1.1325 - val_accuracy: 0.5577\n",
            "Epoch 245/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0388 - accuracy: 0.5878 - val_loss: 1.1292 - val_accuracy: 0.5581\n",
            "Epoch 246/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0343 - accuracy: 0.5897 - val_loss: 1.1279 - val_accuracy: 0.5579\n",
            "Epoch 247/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0433 - accuracy: 0.5864 - val_loss: 1.1271 - val_accuracy: 0.5579\n",
            "Epoch 248/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0363 - accuracy: 0.5894 - val_loss: 1.1231 - val_accuracy: 0.5599\n",
            "Epoch 249/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0341 - accuracy: 0.5916 - val_loss: 1.1329 - val_accuracy: 0.5591\n",
            "Epoch 250/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0301 - accuracy: 0.5919 - val_loss: 1.1257 - val_accuracy: 0.5598\n",
            "Epoch 251/335\n",
            "120/120 [==============================] - 3s 28ms/step - loss: 1.0389 - accuracy: 0.5876 - val_loss: 1.1238 - val_accuracy: 0.5591\n",
            "Epoch 252/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0266 - accuracy: 0.5916 - val_loss: 1.1296 - val_accuracy: 0.5566\n",
            "Epoch 253/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0280 - accuracy: 0.5932 - val_loss: 1.1262 - val_accuracy: 0.5588\n",
            "Epoch 254/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0332 - accuracy: 0.5917 - val_loss: 1.1305 - val_accuracy: 0.5580\n",
            "Epoch 255/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0270 - accuracy: 0.5915 - val_loss: 1.1277 - val_accuracy: 0.5605\n",
            "Epoch 256/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0260 - accuracy: 0.5930 - val_loss: 1.1231 - val_accuracy: 0.5606\n",
            "Epoch 257/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0309 - accuracy: 0.5927 - val_loss: 1.1567 - val_accuracy: 0.5557\n",
            "Epoch 258/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0303 - accuracy: 0.5921 - val_loss: 1.1388 - val_accuracy: 0.5585\n",
            "Epoch 259/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0289 - accuracy: 0.5908 - val_loss: 1.1279 - val_accuracy: 0.5581\n",
            "Epoch 260/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0271 - accuracy: 0.5930 - val_loss: 1.1293 - val_accuracy: 0.5570\n",
            "Epoch 261/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0271 - accuracy: 0.5925 - val_loss: 1.1262 - val_accuracy: 0.5594\n",
            "Epoch 262/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0251 - accuracy: 0.5969 - val_loss: 1.1385 - val_accuracy: 0.5590\n",
            "Epoch 263/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0246 - accuracy: 0.5938 - val_loss: 1.1367 - val_accuracy: 0.5573\n",
            "Epoch 264/335\n",
            "120/120 [==============================] - 4s 32ms/step - loss: 1.0179 - accuracy: 0.5992 - val_loss: 1.1354 - val_accuracy: 0.5593\n",
            "Epoch 265/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0228 - accuracy: 0.5954 - val_loss: 1.1346 - val_accuracy: 0.5589\n",
            "Epoch 266/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0202 - accuracy: 0.5963 - val_loss: 1.1454 - val_accuracy: 0.5587\n",
            "Epoch 267/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0182 - accuracy: 0.5965 - val_loss: 1.1255 - val_accuracy: 0.5618\n",
            "Epoch 268/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0201 - accuracy: 0.5964 - val_loss: 1.1217 - val_accuracy: 0.5631\n",
            "Epoch 269/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0218 - accuracy: 0.5981 - val_loss: 1.1399 - val_accuracy: 0.5605\n",
            "Epoch 270/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0187 - accuracy: 0.5979 - val_loss: 1.1275 - val_accuracy: 0.5624\n",
            "Epoch 271/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0193 - accuracy: 0.5986 - val_loss: 1.1233 - val_accuracy: 0.5627\n",
            "Epoch 272/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0228 - accuracy: 0.5948 - val_loss: 1.1249 - val_accuracy: 0.5626\n",
            "Epoch 273/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0124 - accuracy: 0.5983 - val_loss: 1.1317 - val_accuracy: 0.5617\n",
            "Epoch 274/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0142 - accuracy: 0.5984 - val_loss: 1.1204 - val_accuracy: 0.5650\n",
            "Epoch 275/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0211 - accuracy: 0.5941 - val_loss: 1.1385 - val_accuracy: 0.5618\n",
            "Epoch 276/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0125 - accuracy: 0.6012 - val_loss: 1.1333 - val_accuracy: 0.5598\n",
            "Epoch 277/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0139 - accuracy: 0.6033 - val_loss: 1.1432 - val_accuracy: 0.5599\n",
            "Epoch 278/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0181 - accuracy: 0.5996 - val_loss: 1.1334 - val_accuracy: 0.5645\n",
            "Epoch 279/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0034 - accuracy: 0.6037 - val_loss: 1.1272 - val_accuracy: 0.5639\n",
            "Epoch 280/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0074 - accuracy: 0.6007 - val_loss: 1.1278 - val_accuracy: 0.5628\n",
            "Epoch 281/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0083 - accuracy: 0.6041 - val_loss: 1.1375 - val_accuracy: 0.5598\n",
            "Epoch 282/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0136 - accuracy: 0.6000 - val_loss: 1.1312 - val_accuracy: 0.5611\n",
            "Epoch 283/335\n",
            "120/120 [==============================] - 3s 28ms/step - loss: 1.0102 - accuracy: 0.6015 - val_loss: 1.1365 - val_accuracy: 0.5630\n",
            "Epoch 284/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0067 - accuracy: 0.6024 - val_loss: 1.1476 - val_accuracy: 0.5612\n",
            "Epoch 285/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0101 - accuracy: 0.6032 - val_loss: 1.1376 - val_accuracy: 0.5614\n",
            "Epoch 286/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0030 - accuracy: 0.6042 - val_loss: 1.1333 - val_accuracy: 0.5632\n",
            "Epoch 287/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 1.0070 - accuracy: 0.6030 - val_loss: 1.1503 - val_accuracy: 0.5582\n",
            "Epoch 288/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0005 - accuracy: 0.6053 - val_loss: 1.1300 - val_accuracy: 0.5617\n",
            "Epoch 289/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0009 - accuracy: 0.6067 - val_loss: 1.1332 - val_accuracy: 0.5632\n",
            "Epoch 290/335\n",
            "120/120 [==============================] - 3s 28ms/step - loss: 1.0013 - accuracy: 0.6035 - val_loss: 1.1442 - val_accuracy: 0.5609\n",
            "Epoch 291/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0031 - accuracy: 0.6029 - val_loss: 1.1180 - val_accuracy: 0.5689\n",
            "Epoch 292/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9997 - accuracy: 0.6044 - val_loss: 1.1212 - val_accuracy: 0.5647\n",
            "Epoch 293/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0000 - accuracy: 0.6078 - val_loss: 1.1350 - val_accuracy: 0.5652\n",
            "Epoch 294/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 1.0052 - accuracy: 0.6036 - val_loss: 1.1382 - val_accuracy: 0.5618\n",
            "Epoch 295/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9982 - accuracy: 0.6057 - val_loss: 1.1370 - val_accuracy: 0.5653\n",
            "Epoch 296/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 0.9969 - accuracy: 0.6076 - val_loss: 1.1317 - val_accuracy: 0.5657\n",
            "Epoch 297/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 0.9968 - accuracy: 0.6079 - val_loss: 1.1249 - val_accuracy: 0.5646\n",
            "Epoch 298/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9966 - accuracy: 0.6047 - val_loss: 1.1320 - val_accuracy: 0.5645\n",
            "Epoch 299/335\n",
            "120/120 [==============================] - 4s 32ms/step - loss: 0.9941 - accuracy: 0.6085 - val_loss: 1.1333 - val_accuracy: 0.5656\n",
            "Epoch 300/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9991 - accuracy: 0.6042 - val_loss: 1.1267 - val_accuracy: 0.5678\n",
            "Epoch 301/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9918 - accuracy: 0.6114 - val_loss: 1.1372 - val_accuracy: 0.5652\n",
            "Epoch 302/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9903 - accuracy: 0.6123 - val_loss: 1.1343 - val_accuracy: 0.5659\n",
            "Epoch 303/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9903 - accuracy: 0.6083 - val_loss: 1.1321 - val_accuracy: 0.5669\n",
            "Epoch 304/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9893 - accuracy: 0.6121 - val_loss: 1.1353 - val_accuracy: 0.5662\n",
            "Epoch 305/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 0.9878 - accuracy: 0.6112 - val_loss: 1.1452 - val_accuracy: 0.5618\n",
            "Epoch 306/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9865 - accuracy: 0.6130 - val_loss: 1.1228 - val_accuracy: 0.5683\n",
            "Epoch 307/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9847 - accuracy: 0.6131 - val_loss: 1.1374 - val_accuracy: 0.5657\n",
            "Epoch 308/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9789 - accuracy: 0.6135 - val_loss: 1.1317 - val_accuracy: 0.5665\n",
            "Epoch 309/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9867 - accuracy: 0.6120 - val_loss: 1.1296 - val_accuracy: 0.5656\n",
            "Epoch 310/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9815 - accuracy: 0.6124 - val_loss: 1.1368 - val_accuracy: 0.5638\n",
            "Epoch 311/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9849 - accuracy: 0.6118 - val_loss: 1.1418 - val_accuracy: 0.5615\n",
            "Epoch 312/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9859 - accuracy: 0.6124 - val_loss: 1.1222 - val_accuracy: 0.5675\n",
            "Epoch 313/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9721 - accuracy: 0.6187 - val_loss: 1.1302 - val_accuracy: 0.5635\n",
            "Epoch 314/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9811 - accuracy: 0.6108 - val_loss: 1.1324 - val_accuracy: 0.5675\n",
            "Epoch 315/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9849 - accuracy: 0.6132 - val_loss: 1.1300 - val_accuracy: 0.5658\n",
            "Epoch 316/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9793 - accuracy: 0.6171 - val_loss: 1.1355 - val_accuracy: 0.5656\n",
            "Epoch 317/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9791 - accuracy: 0.6158 - val_loss: 1.1248 - val_accuracy: 0.5672\n",
            "Epoch 318/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 0.9733 - accuracy: 0.6175 - val_loss: 1.1318 - val_accuracy: 0.5664\n",
            "Epoch 319/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9711 - accuracy: 0.6187 - val_loss: 1.1307 - val_accuracy: 0.5672\n",
            "Epoch 320/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9687 - accuracy: 0.6203 - val_loss: 1.1424 - val_accuracy: 0.5629\n",
            "Epoch 321/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 0.9761 - accuracy: 0.6175 - val_loss: 1.1332 - val_accuracy: 0.5662\n",
            "Epoch 322/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 0.9720 - accuracy: 0.6146 - val_loss: 1.1249 - val_accuracy: 0.5670\n",
            "Epoch 323/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9697 - accuracy: 0.6175 - val_loss: 1.1382 - val_accuracy: 0.5661\n",
            "Epoch 324/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 0.9705 - accuracy: 0.6190 - val_loss: 1.1312 - val_accuracy: 0.5670\n",
            "Epoch 325/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9734 - accuracy: 0.6150 - val_loss: 1.1312 - val_accuracy: 0.5662\n",
            "Epoch 326/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9735 - accuracy: 0.6159 - val_loss: 1.1214 - val_accuracy: 0.5676\n",
            "Epoch 327/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 0.9689 - accuracy: 0.6201 - val_loss: 1.1330 - val_accuracy: 0.5646\n",
            "Epoch 328/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9664 - accuracy: 0.6209 - val_loss: 1.1290 - val_accuracy: 0.5664\n",
            "Epoch 329/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9702 - accuracy: 0.6187 - val_loss: 1.1285 - val_accuracy: 0.5666\n",
            "Epoch 330/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 0.9644 - accuracy: 0.6227 - val_loss: 1.1290 - val_accuracy: 0.5683\n",
            "Epoch 331/335\n",
            "120/120 [==============================] - 4s 32ms/step - loss: 0.9701 - accuracy: 0.6182 - val_loss: 1.1293 - val_accuracy: 0.5676\n",
            "Epoch 332/335\n",
            "120/120 [==============================] - 3s 26ms/step - loss: 0.9627 - accuracy: 0.6173 - val_loss: 1.1314 - val_accuracy: 0.5662\n",
            "Epoch 333/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9641 - accuracy: 0.6176 - val_loss: 1.1331 - val_accuracy: 0.5691\n",
            "Epoch 334/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9582 - accuracy: 0.6242 - val_loss: 1.1329 - val_accuracy: 0.5663\n",
            "Epoch 335/335\n",
            "120/120 [==============================] - 3s 27ms/step - loss: 0.9589 - accuracy: 0.6241 - val_loss: 1.1412 - val_accuracy: 0.5668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "continuous-possession"
      },
      "source": [
        "import os\n",
        "\n",
        "def save_history(filename, training_time, history_dict):\n",
        "    # saves history in a csv file from a dictionary\n",
        "    df_history = pd.DataFrame.from_dict(history_dict)\n",
        "    df_history['training_time'] = training_time\n",
        "\n",
        "    # create directory \"history\" if it doesn't exist:\n",
        "    try_mkdir('./history')\n",
        "\n",
        "    path = os.path.join('./history', filename)\n",
        "    df_history.to_csv(path)\n",
        "\n",
        "def save_history_file(path, filename, history_dict):     #for colab only\n",
        "    # saves history in a csv file from a dictionary\n",
        "    df_history = pd.DataFrame.from_dict(history_dict)\n",
        "\n",
        "    path = os.path.join('./sample_data', filename)\n",
        "    df_history.to_csv(path)\n",
        "\n",
        "def try_mkdir(dir):\n",
        "\n",
        "    try:\n",
        "        os.mkdir(dir)\n",
        "    except OSError:\n",
        "        print(\"Creation of the directory %s failed\" % dir)\n",
        "    else:\n",
        "        print(\"Successfully created the directory %s \" % dir)"
      ],
      "id": "continuous-possession",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbN3QqtRlLZm"
      },
      "source": [
        "model_name = 'lstm-2-apr-b512-dropout0-2-bn-dim128-emb-untr-epochs-335-2-layer-ttv-final'"
      ],
      "id": "zbN3QqtRlLZm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "extraordinary-darkness",
        "outputId": "a16dd0c8-13f1-4ce1-b456-8a630da6f0f8"
      },
      "source": [
        "# hist_filename = 'history-lstm-onehot-b32-dropout0-4-bn-dim8.csv'\n",
        "hist_filename = 'history-' + model_name + '.csv'\n",
        "\n",
        "save_history(hist_filename, training_tot, history.history)"
      ],
      "id": "extraordinary-darkness",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creation of the directory ./history failed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpdq6E2Nk9Iv"
      },
      "source": [
        "#save model\n",
        "model_filename = model_name + '.h5'\n",
        "\n",
        "model.save(os.path.join('models', model_filename))"
      ],
      "id": "Qpdq6E2Nk9Iv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "local-fourth",
        "outputId": "502fc648-f79f-45f9-e358-b25bc95d705e"
      },
      "source": [
        "#download history and model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(os.path.join('./history', hist_filename)) \n",
        "files.download(os.path.join('models', model_filename))"
      ],
      "id": "local-fourth",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0db82519-fe15-470f-bfd7-8ecd96712211\", \"history-lstm-2-apr-b512-dropout0-2-bn-dim128-emb-untr-epochs-335-2-layer-ttv-final.csv\", 32699)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_264b58ba-82e6-439c-a567-25d43af5fa7c\", \"lstm-2-apr-b512-dropout0-2-bn-dim128-emb-untr-epochs-335-2-layer-ttv-final.h5\", 12718680)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOMRBEH6at5N"
      },
      "source": [
        "#Confusion Matrix"
      ],
      "id": "hOMRBEH6at5N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "documentary-edition"
      },
      "source": [
        "# inference on validation dataset\n",
        "\n",
        "test_st = time.time()\n",
        "y_validation_predicted_np = model.predict_on_batch(encodings_validation)  # numpy array\n",
        "\n",
        "test_tot = time.time() - test_st"
      ],
      "id": "documentary-edition",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "welcome-proportion",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58d0e8d2-f33a-483b-de5f-c7f16b71f7b8"
      },
      "source": [
        "test_tot"
      ],
      "id": "welcome-proportion",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2086613178253174"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvpUJmDkgyoV"
      },
      "source": [
        "def get_prediction(arr):\n",
        "      return arr.argmax()"
      ],
      "id": "mvpUJmDkgyoV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN5GuOXKgMwF"
      },
      "source": [
        "y_validation_predicted = np.apply_along_axis(get_prediction, 1, y_validation_predicted_np)"
      ],
      "id": "uN5GuOXKgMwF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Dbh66Zdhdj0V",
        "outputId": "bb774320-a5de-4f1b-f485-12e003771034"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "conf_matrix = confusion_matrix(y_validation, y_validation_predicted)\n",
        "fig = sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
        "fig.set(xlabel = 'Predicted labels', ylabel = 'True labels')\n",
        "\n",
        "plt.show()"
      ],
      "id": "Dbh66Zdhdj0V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2m5tmSOY95i"
      },
      "source": [
        ""
      ],
      "id": "F2m5tmSOY95i",
      "execution_count": null,
      "outputs": []
    }
  ]
}